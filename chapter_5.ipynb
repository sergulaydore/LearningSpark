{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Saving your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far our examples have loaded and saved all of their data from a native collection and regular files, but odds are that your data doesn't fit on a single machine, so it's time to explore our options for loading and saving.\n",
    "\n",
    "Spark supports a wide range of input and output sources, partly because it builds on the ecosystem avaialble for Hadoop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we load a single text file as an RDD, each input line becomes an element in the RDD. We can also load multiple whole text files at the same time into a pair RDD, with the key being the name and the value being the contents of each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading a text file\n",
    "input_file = sc.textFile(\"file:/Users/sergulaydore/Downloads/nyc16_bigdata1-master-3/week2b_thursday/shakespeare.txt\")\n",
    "# we can control the number of partitions by specifying minPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our files are small enough, then we can use the SparkContext.wholeTextFiles() method and get back a pair RDD where the key is the name of the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving as a text file\n",
    "result.saveAsTextFile(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Loading unstructured JSON in Python\n",
    "import json\n",
    "data = input.map(lambda x: json.loads(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving JSON - say we are running a promotion for people who love pandas\n",
    "(data.filter(lambda x: x[\"lovesPandas\"]).map(lambda x: json.dumps(x))).saveAsTextFile(outputFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comma-Separated Values and Tab-Separated Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, in addition to writing CSV loading code by hand, there is a package on http:/www.spark-packages.org called spark-csv to load csv data as a Spark SQL data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading CSV with textFile()\n",
    "import csv\n",
    "import StringIO\n",
    "def loadRecord(line):\n",
    "    \"\"\"Parse a CSV line\"\"\"\n",
    "    my_input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(my_input, fieldnames = [\"name\", \"favoriteAnimal\"])\n",
    "    return reader.next()\n",
    "my_input = sc.TextFile(inputFile).map(loadRecord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Writing CSV\n",
    "def writeRecords(records):\n",
    "    \"\"\"Write out CSV lines\"\"\"\n",
    "    output = StringIO.StringIO()\n",
    "    writer = csv.DictWriter(output, fieldnames = [\"name\", \"favoriteAnimal\"])\n",
    "    for record in records:\n",
    "        writer.writerow(record)\n",
    "    return [output.getvalue()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SequenceFiles are a popular Hadoop format composed of flat files with key/value pairs. SequenceFiles consist of elements that implement Hadoop's Writable interface, as Hadoop uses a custom serialization framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark has a specialized API for reading in SequenceFiles. On the SparkContext we can call sequenceFile(path, keyClass, valueClass, minPartitions). Let's consider loading people and the number of pandas they have seen from a SequenceFile. In this case our keyClass would be text and our valueClass would be IntWritable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading a SequenceFile\n",
    "data = sc.sequenceFile(inFile,\n",
    "                      \"org.apache.hadoop.io.Text\", \"org.apache.hadoop.io.IntWritable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object files are deceptively simple wrapper around SequenceFiles that allows us to save our RDDs containing just values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Input and Output Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get back to this chapter as you need different data types and file systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
